<h1 id="loganalysis">log.analysis</h1>



<h1 id="features">Features</h1>

<ol>
<li>hadoop based ETL platform</li>
<li>oozie as the workflow and coordination engine</li>
<li>commands can be run under mapreduce model or single process model</li>
</ol>



<h1 id="sample-flow">Sample Flow</h1>

<p>PDE ETL Flow <br>
<img src="https://raw.githubusercontent.com/phillipcheng/log.analysis/master/pde.analysis/pic/flow1.png" alt="PDE ETL Flow" title=""> <br>
SGSIWF ETL Flow <br>
<img src="https://raw.githubusercontent.com/phillipcheng/log.analysis/master/mtccore/pic/flow1.png" alt="enter image description here" title=""></p>

<h1 id="reusable-commands">Reusable Commands</h1>

<h2 id="1-csv-file-transformation">1. CSV file transformation</h2>

<h3 id="columns-merge">Columns Merge</h3>

<pre><code>    expression supported
    Example:
    concatenate column 68 with column 69, and joined by '-'
    merge.idx=68:69:(a.concat('-')).concat(b)

    column 0 is weekly number since epoch, and column 1 is the time part
    following expression caculate the epoch time by merging these two columns
    merge.idx=0:1:(Number(a)*7*24*3600 + Number(b)).toString()
</code></pre>

<h3 id="column-split">Column Split</h3>

<pre><code>    Example:
    split the column 3 into 2 fields by first '.'
    split.idx=3:.
</code></pre>

<h3 id="single-column-update">Single Column Update</h3>

<pre><code>    Example:
    remove the '.' in column 2 and remove the '\' in column 3
    update.idx=2:a.replace('\.',''),3:a.replace('\\','');
</code></pre>

<h3 id="add-transformed-file-name-to-row-data">Add Transformed file name to row data</h3>

<h3 id="skip-header">Skip header</h3>

<h3 id="row-ends-with-comma">Row Ends with comma</h3>

<h3 id="row-validation-support">Row Validation support</h3>

<h2 id="2-seed-generation-cmd">2. Seed Generation Cmd</h2>

<pre><code>    mapreduce (seed) input generation for non csv files
    Example:
    working.folder=hdfs://192.85.247.104:19000/pde
    input.folder=rawinput
    input.files.threshold=-1
    output.seed.folder=seedinput
</code></pre>



<h2 id="3-sftp-fetch-files-cmd">3. SFTP fetch files Cmd</h2>

<pre><code>    copy files to hdfs from sftp servers
</code></pre>



<h2 id="4-dynamic-schema-cmd">4. Dynamic Schema Cmd</h2>

<pre><code>    generate or update the schema based on the input xml files
</code></pre>



<h2 id="5-dynamic-sql-generationexecution-cmd">5. Dynamic Sql Generation/Execution Cmd</h2>

<pre><code>    used together with dynamic schema Cmd
</code></pre>



<h2 id="6-backup-cmd">6. Backup Cmd</h2>

<pre><code>    zip all the raw files and intermediate files and backup to data lake
</code></pre>



<h2 id="7-kcv-to-csv-transformation-cmd">7. KCV to CSV Transformation Cmd</h2>

<pre><code>    kcv looks like
    key:Value key:Value
</code></pre>



<h2 id="8-load-database-cmd">8. Load Database Cmd</h2>

<pre><code>    load database with csv file
    vertica hdfs connector support
</code></pre>



<h2 id="9-shell-command">9. Shell Command</h2>

<pre><code>    invoke shell command with parameters
</code></pre>



<h2 id="10-upload-command">10. Upload Command</h2>

<pre><code>    copy load files to hdfs
</code></pre>