<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns:ssh="uri:oozie:ssh-action:0.2" xmlns:shell="uri:oozie:shell-action:0.3" xmlns="uri:oozie:workflow:0.5" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="uri:oozie:workflow:0.5
C:\Java\oozie\wf-0.5.xsd
uri:oozie:shell-action:0.3
C:\Java\oozie\shell-action-0.3.xsd
uri:oozie:ssh-action:0.2
C:\Java\oozie\ssh-action-0.2.xsd" name="atterad">
	<!-- <start to="SftpGetRawFiles"/> -->
	<start to="CsvTransform2"/>
		<action name="SftpGetRawFiles">
		<java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>etl.engine.ETLCmdMain</main-class>
            <arg>etl.cmd.SftpCmd</arg>
            <arg>${wf:id()}</arg>
            <arg>/atterad/etlcfg/erad.sftp.properties</arg>
            <arg>unused</arg>
            <arg>unused</arg>
            <capture-output/>
        </java>
		<ok to="CsvTransform1"/>
		<error to="fail"/>
	</action>
	<kill name="fail">
		<message>Java failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
	</kill>
	<action name="CsvTransform1">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="/atterad/input2/all2"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>etl.engine.InvokeMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.NLineInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.input.lineinputformat.linespermap</name>
                    <value>100000</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.TextOutputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.hadoop.io.NullWritable</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>/atterad/input/all2.csv</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>/atterad/input2/all2</value>
                </property>
                <property>
                    <name>cmdClassName</name>
                    <value>etl.cmd.transform.CsvTransformCmd</value>
                </property>
                <property>
                    <name>wfid</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>staticConfigFile</name>
                    <value>/atterad/etlcfg/erad.csvtrans1.properties</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="FsCmd"/>
        <error to="fail"/>
    </action>
	<action name="FsCmd">
		<fs>
		  <move source='${nameNode}/atterad/input/all.csv' target='/atterad/input2/all.csv'/>
		  <move source='${nameNode}/atterad/input/phxall4.csv' target='/atterad/input2/phxall4.csv'/>
		  <move source='${nameNode}/atterad/input/phxall5.csv' target='/atterad/input2/phxall5.csv'/>
		</fs>
		<ok to="CsvTransform2"/>
		<error to="fail"/>
	</action>
	<action name="CsvTransform2">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="/atterad/input3/"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>etl.engine.InvokeMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.NLineInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.input.lineinputformat.linespermap</name>
                    <value>100000</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.TextOutputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.hadoop.io.NullWritable</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>/atterad/input2/</value>
                </property>
                <property>
                    <name>mapred.input.dir.recursive</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>/atterad/input3/</value>
                </property>
                <property>
                    <name>cmdClassName</name>
                    <value>etl.cmd.transform.CsvTransformCmd</value>
                </property>
                <property>
                    <name>wfid</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>staticConfigFile</name>
                    <value>/atterad/etlcfg/erad.csvtrans2.properties</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="loadCsvCmd"/>
        <error to="fail"/>
    </action>
	<action name="loadCsvCmd">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>etl.engine.ETLCmdMain</main-class>
            <arg>etl.cmd.LoadDataCmd</arg>
            <arg>${wf:id()}</arg>
            <arg>/pde/etlcfg/erad.load.properties</arg>
            <arg>unused</arg>
            <arg>unused</arg>
            <capture-output/>
        </java>
        <ok to="end"/>
        <error to="fail"/>
    </action>
	<end name="end"/>
</workflow-app>
