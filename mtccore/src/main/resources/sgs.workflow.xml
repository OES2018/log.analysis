<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns:ssh="uri:oozie:ssh-action:0.2" xmlns:shell="uri:oozie:shell-action:0.3" xmlns="uri:oozie:workflow:0.5" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="uri:oozie:workflow:0.5
C:\Java\oozie\wf-0.5.xsd
uri:oozie:shell-action:0.3
C:\Java\oozie\shell-action-0.3.xsd
uri:oozie:ssh-action:0.2
C:\Java\oozie\ssh-action-0.2.xsd" name="mtccore">
	<start to="SftpGetRawFiles"/>
	<kill name="fail">
        <message>Java failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
	<action name="SftpGetRawFiles">
		<map-reduce>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<configuration>
				<property>
					<name>mapred.mapper.new-api</name>
					<value>true</value>
				</property>
				<property>
					<name>mapred.reducer.new-api</name>
					<value>true</value>
				</property>
				<property>
					<name>mapreduce.job.map.class</name>
					<value>etl.engine.InvokeMapper</value>
				</property>
				<property>
					<name>mapreduce.job.inputformat.class</name>
					<value>org.apache.hadoop.mapreduce.lib.input.NLineInputFormat</value>
				</property>
				<property>
					<name>mapreduce.input.lineinputformat.linespermap</name>
					<value>1</value>
				</property>
				<property>
					<name>mapreduce.job.outputformat.class</name>
					<value>org.apache.hadoop.mapreduce.lib.output.NullOutputFormat</value>
				</property>
				<property>
					<name>mapreduce.task.timeout</name>
					<value>0</value>
				</property>
				<property>
					<name>mapreduce.input.fileinputformat.inputdir</name>
					<value>/mtccore/etlcfg/sgsiwf.sftp.map.properties</value>
				</property>
				<property>
					<name>cmdClassName</name>
					<value>etl.cmd.SftpCmd</value>
				</property>
				<property>
					<name>wfid</name>
					<value>${wf:id()}</value>
				</property>
				<property>
					<name>staticConfigFile</name>
					<value>/mtccore/etlcfg/sgsiwf.sftp.properties</value>
				</property>
			</configuration>
		</map-reduce>
		<ok to="SchemaUpdateFromXml"/>
		<error to="fail"/>
	</action>
	<action name="SchemaUpdateFromXml">
		<java>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<main-class>etl.engine.ETLCmdMain</main-class>
			<arg>etl.cmd.SchemaFromXmlCmd</arg>
			<arg>${wf:id()}</arg>
			<arg>/mtccore/etlcfg/sgsiwf.schemaFromXml.properties</arg>
			<arg>${nameNode}</arg>
		</java>
		<ok to="GenCsv"/>
		<error to="fail"/>
	</action>
	<action name="GenCsv">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="/mtccore/csvdata/csv/${wf:id()}"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>etl.engine.InvokeReducerMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduce.class</name>
                    <value>etl.engine.InvokeNullableReducer</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>etl.util.FilenameInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>/mtccore/xmldata/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>/mtccore/csvdata/csv/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.map.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.map.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>cmdClassName</name>
                    <value>etl.cmd.XmlToCsvCmd</value>
                </property>
                <property>
                    <name>wfid</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>staticConfigFile</name>
                    <value>/mtccore/etlcfg/sgsiwf.schemaFromXml.properties</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="LogGenCsvInfo"/>
        <error to="fail"/>
    </action>
    <action name="LogGenCsvInfo">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>etl.engine.ETLCmdMain</main-class>
            <arg>etl.cmd.SendLogCmd</arg>
            <arg>${wf:id()}</arg>
            <arg>/mtccore/etlcfg/sgsiwf.sendlog.properties</arg>
            <arg>${nameNode}</arg>
            <arg>GenCsv</arg>
            <arg>${hadoop:counters("GenCsv")["org.apache.hadoop.mapred.Task$Counter"]["MAP_OUTPUT_RECORDS"]}</arg>
            <arg>${hadoop:counters("GenCsv")["org.apache.hadoop.mapred.Task$Counter"]["REDUCE_OUTPUT_RECORDS"]}</arg>
        </java>
        <ok to="UpdateAggrSchema"/>
        <error to="fail"/>
    </action>
	<action name="UpdateAggrSchema">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>etl.engine.ETLCmdMain</main-class>
            <arg>etl.cmd.CsvAggregateCmd</arg>
            <arg>${wf:id()}</arg>
            <arg>/mtccore/etlcfg/sgsiwf.aggr.properties</arg>
            <arg>${nameNode}</arg>
        </java>
        <ok to="AggregateVlr"/>
        <error to="fail"/>
    </action>
    <action name="AggregateVlr">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="/mtccore/csvdata/aggr/${wf:id()}"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>etl.engine.InvokeReducerMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduce.class</name>
                    <value>etl.engine.InvokeReducer</value>
                </property>
                <property>
                    <name>mapreduce.output.textoutputformat.separator</name>
                    <value>,</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>/mtccore/csvdata/csv/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>/mtccore/csvdata/aggr/${wf:id()}</value>
                </property>
                <property>
                    <name>mapreduce.map.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.map.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>cmdClassName</name>
                    <value>etl.cmd.CsvAggregateCmd</value>
                </property>
                <property>
                    <name>wfid</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>staticConfigFile</name>
                    <value>/mtccore/etlcfg/sgsiwf.aggr.properties</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="LogAggrVlrInfo"/>
        <error to="fail"/>
    </action>
    <action name="LogAggrVlrInfo">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>etl.engine.ETLCmdMain</main-class>
            <arg>etl.cmd.SendLogCmd</arg>
            <arg>${wf:id()}</arg>
            <arg>/mtccore/etlcfg/sgsiwf.sendlog.properties</arg>
            <arg>${nameNode}</arg>
            <arg>AggregateVlr-Info</arg>
            <arg>${hadoop:counters("AggregateVlr")["org.apache.hadoop.mapred.Task$Counter"]["MAP_OUTPUT_RECORDS"]}</arg>
            <arg>${hadoop:counters("AggregateVlr")["org.apache.hadoop.mapred.Task$Counter"]["REDUCE_OUTPUT_RECORDS"]}</arg>
        </java>
        <ok to="UpdateTransformSchema"/>
        <error to="fail"/>
    </action>
    <action name="UpdateTransformSchema">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>etl.engine.ETLCmdMain</main-class>
            <arg>etl.cmd.CsvTransformCmd</arg>
            <arg>${wf:id()}</arg>
            <arg>/mtccore/etlcfg/sgsiwf.trans.properties</arg>
            <arg>${nameNode}</arg>
        </java>
        <ok to="TransformVlr"/>
        <error to="fail"/>
    </action>
    <action name="TransformVlr">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="/mtccore/csvdata/trans/${wf:id()}"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>etl.engine.InvokeMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.TextInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.TextOutputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.hadoop.io.NullWritable</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>/mtccore/csvdata/aggr/${wf:id()}/PoolType_vlr_-r-*</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>/mtccore/csvdata/trans/${wf:id()}</value>
                </property>
                <property>
                    <name>cmdClassName</name>
                    <value>etl.cmd.CsvTransformCmd</value>
                </property>
                <property>
                    <name>wfid</name>
                    <value>${wf:id()}</value>
                </property>
                <property>
                    <name>staticConfigFile</name>
                    <value>/mtccore/etlcfg/sgsiwf.trans.properties</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="LogTransformVlrInfo"/>
        <error to="fail"/>
    </action>
    <action name="LogTransformVlrInfo">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>etl.engine.ETLCmdMain</main-class>
            <arg>etl.cmd.SendLogCmd</arg>
            <arg>${wf:id()}</arg>
            <arg>/mtccore/etlcfg/sgsiwf.sendlog.properties</arg>
            <arg>${nameNode}</arg>
            <arg>TransformVlr-Info</arg>
            <arg>${hadoop:counters("TransformVlr")["org.apache.hadoop.mapred.Task$Counter"]["MAP_OUTPUT_RECORDS"]}</arg>
            <arg>${hadoop:counters("TransformVlr")["org.apache.hadoop.mapred.Task$Counter"]["REDUCE_OUTPUT_RECORDS"]}</arg>
        </java>
        <ok to="copyVlrMerge"/>
        <error to="fail"/>
    </action>
    <action name="copyVlrMerge">
         <fs>
            <move 
                source='${nameNode}/mtccore/csvdata/trans/${wf:id()}/part-r-00000' 
                target='/mtccore/csvdata/csv/${wf:id()}/PoolType_vlr_aggr-r-00000'/>
            <move 
                source='${nameNode}/mtccore/csvdata/aggr/${wf:id()}/MME_PoolType_wss7_-r-00000' 
                target='/mtccore/csvdata/csv/${wf:id()}/MME_PoolType_wss7_aggr-r-00000'/>
            <move 
                source='${nameNode}/mtccore/csvdata/aggr/${wf:id()}/MME_PoolType_vlr_-r-00000' 
                target='/mtccore/csvdata/csv/${wf:id()}/MME_PoolType_vlr_aggr-r-00000'/>
            <move 
                source='${nameNode}/mtccore/csvdata/aggr/${wf:id()}/PoolType_vlr_TechnologyType_-r-00000' 
                target='/mtccore/csvdata/csv/${wf:id()}/PoolType_vlr_TechnologyType_aggr-r-00000'/>
            <delete path='${nameNode}/mtccore/csvdata/trans/${wf:id()}'/>
            <delete path='${nameNode}/mtccore/csvdata/aggr/${wf:id()}'/>
         </fs>
        <ok to="LoadData"/>
        <error to="fail"/>
    </action>
	<action name="LoadData">
		<java>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<main-class>etl.engine.ETLCmdMain</main-class>
			<arg>etl.cmd.LoadDataCmd</arg>
			<arg>${wf:id()}</arg>
			<arg>/mtccore/etlcfg/sgsiwf.loaddata.properties</arg>
			<arg>${nameNode}</arg>
		</java>
		<ok to="BackupData"/>
		<error to="fail"/>
	</action>
	<action name="BackupData">
		<java>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<main-class>etl.engine.ETLCmdMain</main-class>
			<arg>etl.cmd.BackupCmd</arg>
			<arg>${wf:id()}</arg>
			<arg>/mtccore/etlcfg/sgsiwf.backup.properties</arg>
			<arg>${nameNode}</arg>
		</java>
		<ok to="cleanup"/>
		<error to="fail"/>
	</action>
	<action name="cleanup">
         <fs>
            <delete path='${nameNode}/mtccore/csvdata/${wf:id()}'/>
         </fs>
        <ok to="end"/>
        <error to="fail"/>
    </action>
	<end name="end"/>
</workflow-app>
